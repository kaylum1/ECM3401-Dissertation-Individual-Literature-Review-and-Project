

# Passive Vulnerability Cross-Reference Scanner
# =============================================
#
# Identifies server, CMS, and common JS libraries by probing headers and
# script URLs, then cross-references with NVD to score any known CVEs.

import re
import requests
from urllib.parse import urlparse
from bs4 import BeautifulSoup

# — patterns for detecting tech in headers or script srcs —
TECH_PATTERNS = {
    'Server':       r'Server:\s*(.+)',
    'X-Powered-By': r'X-Powered-By:\s*(.+)',
    'jQuery':       r'jquery-(\d+\.\d+\.\d+)\.min\.js',
    'Bootstrap':    r'bootstrap-(\d+\.\d+\.\d+)\.min\.js',
    'WordPress':    r'/wp-content/',
    'Drupal':       r'/sites/all/',
    'Joomla':       r'/media/system/js/',
}

# — NVD search endpoint & severity→penalty mapping —
NVD_SEARCH    = 'https://services.nvd.nist.gov/rest/json/cves/1.0?keyword='
SEV_PENALTIES = {
    'critical': 4,
    'high':     3,
    'medium':   2,
    'low':      1,
}

def get_headers(url: str) -> dict:
    """HEAD request to grab response headers; returns {} on error."""
    try:
        resp = requests.head(url, allow_redirects=True, timeout=10)
        return resp.headers
    except requests.RequestException:
        return {}

def get_content(url: str) -> str | None:
    """GET request for page HTML; returns None on failure."""
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException:
        return None

def detect_technologies(base_url: str, headers: dict, html: str) -> dict:
    """
    Scan headers and <script src=> tags to identify technologies.
    Returns a dict {tech_name: version_or_marker}.
    """
    found = {}
    # look in headers
    for hdr_name, hdr_val in headers.items():
        entry = f'{hdr_name}: {hdr_val}'
        for tech, pat in TECH_PATTERNS.items():
            m = re.search(pat, entry, re.IGNORECASE)
            if m:
                found[tech] = m.group(1)
    # look in script tags
    soup = BeautifulSoup(html, 'html.parser')
    for tag in soup.find_all('script', src=True):
        src = tag['src']
        for tech, pat in TECH_PATTERNS.items():
            m = re.search(pat, src, re.IGNORECASE)
            if m:
                found[tech] = m.group(1)
    return found

def check_vulnerabilities(tech_map: dict) -> tuple[int, list[str]]:
    """
    For each detected technology, query NVD and deduct points per CVE found.
    Returns (score, list_of_notes).
    """
    score = 10
    notes = []

    for tech, version in tech_map.items():
        query = f'{tech} {version}' if version else tech
        url = NVD_SEARCH + requests.utils.quote(query)
        try:
            resp = requests.get(url, timeout=10)
            data = resp.json()
        except Exception:
            continue

        items = data.get('result', {}).get('CVE_Items', [])
        if not items:
            continue

        # take up to three CVEs per tech to keep it brief
        for cve in items[:3]:
            sev = cve.get('impact', {}) \
                     .get('baseMetricV3', {}) \
                     .get('cvssV3', {}) \
                     .get('baseSeverity', 'unknown') \
                     .lower()
            penalty = SEV_PENALTIES.get(sev, 1)
            score -= penalty
            cve_id = cve.get('cve', {}) \
                        .get('CVE_data_meta', {}) \
                        .get('ID', 'N/A')
            notes.append(f'{tech}: {cve_id} ({sev})')

    # clamp to [1,10]
    score = max(1, min(10, score))
    if score == 10:
        notes.append('No CVEs found for detected tech.')
    elif score < 5:
        notes.append('High risk: multiple CVEs discovered.')
    else:
        notes.append('Moderate risk: some CVEs detected.')
    return score, notes

def get_base_url(raw: str) -> str:
    """Normalize input to a scheme://host URL."""
    if '://' not in raw:
        raw = 'https://' + raw
    p = urlparse(raw)
    return f'{p.scheme}://{p.netloc}'

def analyze_vulnerabilities(raw_url: str) -> tuple[int, str]:
    """
    End-to-end wrapper:  
    1) normalize URL, 2) fetch headers & HTML,  
    3) detect tech, 4) check NVD,  
    5) return (score, semicolon-joined notes).
    """
    base = get_base_url(raw_url)
    hdrs = get_headers(base)
    html = get_content(base)
    if not hdrs or html is None:
        return 1, 'Error fetching headers or content.'
    tech = detect_technologies(base, hdrs, html)
    if not tech:
        return 10, 'No recognizable technologies found.'
    score, notes = check_vulnerabilities(tech)
    return score, '; '.join(notes)

def main():
    import argparse
    p = argparse.ArgumentParser(
        description='Scan site tech against NVD vulnerability feed'
    )
    p.add_argument('-u', '--url', required=True, help='Target website')
    args = p.parse_args()

    base = get_base_url(args.url)
    print(f'\nScanning: {base}\n')
    hdrs = get_headers(base)
    html = get_content(base)
    tech = detect_technologies(base, hdrs, html)
    print('Detected tech:', tech or 'None')

    score, details = analyze_vulnerabilities(base)
    print(f'\nFinal Score: {score}/10\n')
    for entry in details.split('; '):
        print('-', entry)

if __name__ == '__main__':
    main()
